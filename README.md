# GCP Data Pipeline with Composer & BigQuery

This project demonstrates a complete GCP-based ETL workflow using **Cloud Composer (Airflow)**, **Google Cloud Storage (GCS)**, and **BigQuery**.

---

## ğŸš€ Project Highlights
- **Airflow DAG**: Automates CSV file ingestion from GCS to BigQuery.
- **Terraform**: Provisions a Composer 2 environment using Infrastructure as Code.
- **GitHub Actions**: CI/CD pipeline for deploying Airflow DAGs to Composer.

---

## ğŸ“ Project Structure
```bash
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ gcs_to_bigquery_etl.py      # Airflow DAG
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.csv             # Sample input file
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                     # Composer infra config
â”‚   â”œâ”€â”€ variables.tf                # Variables for Composer
â”‚   â””â”€â”€ terraform.tfvars            # Your specific values
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ deploy-composer.yml         # GitHub Actions workflow
â”œâ”€â”€ README.md
```

---

## ğŸŒ Prerequisites
- A Google Cloud Project
- Enabled services: Composer, GCS, BigQuery
- A GCS bucket to upload DAGs (Composer managed)

---

## ğŸ§± Terraform Deployment
```bash
cd terraform
terraform init
terraform apply -var-file="terraform.tfvars"
```

---

## â›“ GitHub Actions CI/CD
Add the following **GitHub Secrets**:
- `GCP_PROJECT_ID`
- `GCP_SA_KEY` (JSON credentials string)
- `COMPOSER_BUCKET`

GitHub Actions will auto-sync any changes in `dags/` to the Composer bucket.

---

## ğŸ“Š Output Example (in BigQuery)
| id | name     | age |
|----|----------|-----|
| 1  | Alice    | 25  |
| 2  | Bob      | 30  |

---

## ğŸ’¡ Use Cases
- Cloud-native ETL pipelines
- Scheduled data ingestion into BigQuery
- Infrastructure-as-Code with Terraform
- CI/CD for Airflow DAGs

---

## ğŸ‘¨â€ğŸ’» Author
**Kewal Dangore**  
[LinkedIn](https://www.linkedin.com/in/kewal-dangore)  
DevOps & GCP Engineer

---

## ğŸ“Œ Tags
`#gcp` `#composer` `#bigquery` `#terraform` `#ci-cd` `#airflow`
